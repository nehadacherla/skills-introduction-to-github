{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEMkz43tCs3QNOMBVxydVz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nehadacherla/skills-introduction-to-github/blob/main/DataProject1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Project 1\n",
        "# Neha Dacherla and Christine Lee\n",
        "\n",
        "#!pip install pandas\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import sqlite3\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Fetch/download/retrieve a remote data file by URL\n",
        "url = 'https://raw.githubusercontent.com/nehadacherla/skills-introduction-to-github/refs/heads/main/candy-data.csv'\n",
        "response = requests.get(url)\n",
        "with open('candy-data.csv', 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Generate brief summary of data file ingestion with number of records and number of columns\n",
        "def generate_summary(datafile):\n",
        "    data = pd.read_csv(datafile)\n",
        "    data.head(10)\n",
        "    num_records = len(data)\n",
        "    num_columns = len(data.columns)\n",
        "    print(f\"Data Ingestion Summary:\\nNumber of Records: {num_records}\\nNumber of Columns: {num_columns}\")\n",
        "\n",
        "generate_summary('candy-data.csv')\n",
        "\n",
        "# Convert\n",
        "def convert_to_target(target_format):\n",
        "  df = pd.read_csv('candy-data.csv')\n",
        "\n",
        "# Converting to json from CSV\n",
        "  if target_format == 'json':\n",
        "    df.to_json('candy-data.json', orient='records')\n",
        "    print(\"CSV file converted to JSON and saved as data.json\")\n",
        "\n",
        "# Converting to excel from CSV\n",
        "  elif target_format == 'excel':\n",
        "    df.to_excel('candy-data.xlsx', index=False)\n",
        "    print(\"CSV file converted to Excel and saved as data.xlsx\")\n",
        "\n",
        "# Converting to sqlite from CSV\n",
        "  elif target_format == 'sqlite':\n",
        "    connect = sqlite3.connect('candy_data.db')\n",
        "    df.to_sql('candy_data', connect, if_exists='replace', index=False)\n",
        "    print(\"CSV file converted to SQLite and saved as candy_data.db\")\n",
        "\n",
        "# Keeping a CSV file in the given format\n",
        "  elif target_format == 'csv':\n",
        "    print(\"file is already in CSV format\")\n",
        "\n",
        "# Mitigating errors if the user asks for an invalid target format for conversion\n",
        "  else:\n",
        "    print(\"Invalid target format. Please choose 'json', 'excel', or 'sqlite'.\")\n",
        "\n",
        "# User inputs target format\n",
        "print(\"Choose a target format for conversion: json, excel, sqlite\")\n",
        "target_format = input(\"Enter the target format: \").lower()\n",
        "\n",
        "convert_to_target(target_format)\n",
        "\n",
        "# Modify & Store\n",
        "def modify_format(target_format):\n",
        "  df = pd.read_csv('candy-data.csv')\n",
        "\n",
        "# Dropped the sugarpercent column and the pricepercent column\n",
        "  if 'sugarpercent' in df.columns:\n",
        "    df = df.drop(columns=['sugarpercent'])\n",
        "    print(\"Column 'sugarpercent' removed successfully.\")\n",
        "  else:\n",
        "    print(\"Column 'sugarpercent' not found in the DataFrame.\")\n",
        "\n",
        "  if 'pricepercent' in df.columns:\n",
        "    df = df.drop(columns=['pricepercent'])\n",
        "    print(\"Column 'pricepercent' removed successfully.\")\n",
        "  else:\n",
        "    print(\"Column 'pricepercent' not found in the DataFrame.\")\n",
        "\n",
        "# Stored the converted file as a CSV\n",
        "  df.to_csv('candy-data.csv', index=False)\n",
        "  print(\"CSV file modified and saved as 'modified_candy_data.csv'\")\n",
        "\n",
        "modify_format(target_format)\n",
        "\n",
        "# Generate a brief summary of the post processing\n",
        "def generate_summary2(datafile):\n",
        "    data = pd.read_csv(datafile)\n",
        "    num_records = len(data)\n",
        "    num_columns = len(data.columns)\n",
        "    print(f\"Post Processing Summary:\\nNumber of Records: {num_records}\\nNumber of Columns: {num_columns}\")\n",
        "\n",
        "generate_summary2('candy-data.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtezHJOrTeN8",
        "outputId": "a2a717f4-9b9a-4325-f5bf-dd43cbee5e4f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Ingestion Summary:\n",
            "Number of Records: 85\n",
            "Number of Columns: 13\n",
            "Choose a target format for conversion: json, excel, sqlite\n",
            "Enter the target format: sqlite\n",
            "CSV file converted to SQLite and saved as candy_data.db\n",
            "Column 'sugarpercent' removed successfully.\n",
            "Column 'pricepercent' removed successfully.\n",
            "CSV file modified and saved as 'modified_candy_data.csv'\n",
            "Post Processing Summary:\n",
            "Number of Records: 85\n",
            "Number of Columns: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Project 1 Reflection by Neha Dacherla and Christine Lee**\n",
        "\n",
        "In our first data project, we encountered challenges and successes, as well as lessons on the applications of ETL pipelines. One of our biggest challenges was making the code cohesive to solve all the benchmarks without being repetitive and ordering them in a fashion that makes logical sense. It was hard to assess how and when to execute the summary of the data file ingestion, for example. Another challenge we faced was figuring out how to store the converted file to a disk or SQL database. We were also initially unsure of whether the converted file should be the modified version or not.\n",
        "\n",
        "Fetching/downloading/retrieving the data file by URL was easier than we expected, as we’ve had practice with it in class and for assignments. We debated whether to use a local file or a URL, but chose the URL option to improve ease of access for other users. Additionally, converting the data source to an output of the user’s choice was easier than expected as well, as we just went over this in class. Modifying the columns was easy once we decided which columns we wanted to remove.\n",
        "\n",
        "A utility similar to the  ETL pipeline we built would be applicable to a lot of other data-heavy projects by automating the data gathering, transforming it to a usable format, and storing it in the appropriate data structure. The data ingestion from various sources is applicable because data projects often rely on pulling data from CSV/JSON files, databases, APIs, and more data formats and this pipeline provides scalability with numerous sources and allows for effective data cleaning and transformation. The scalability is especially important because in real-world projects where quick processing is necessary for efficient systems, the pipeline’s ability to handle big data is a key differentiator. The core ETL process also remains relatively uniform for most data projects so data scientists can save time and manual work in new project development. The integration with databases and systems makes data more easily accessible for queries, machine learning, reporting, and more. The error handling measures that are also put up are critical to detecting issues with bad data when analyzing projects and provides a checks system to prevent misinformed data analysis.\n"
      ],
      "metadata": {
        "id": "7jbMgZh1dird"
      }
    }
  ]
}